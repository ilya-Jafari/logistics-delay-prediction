import streamlit as st
import pandas as pd
import joblib
import ollama
import requests
from bs4 import BeautifulSoup
import pm4py
from pm4py.objects.conversion.log import converter as log_converter

# Û±. ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØµÙØ­Ù‡ (Ø­ØªÙ…Ø§Ù‹ Ø¨Ø§ÛŒØ¯ Ø§ÙˆÙ„ÛŒÙ† Ø¯Ø³ØªÙˆØ± Ø¨Ø§Ø´Ø¯)
st.set_page_config(page_title="2026 Logistics Intelligence Hub", page_icon="ğŸŒ", layout="wide")

# --- Û². ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ (AI, Mining, Process) ---

def fetch_live_news():
    try:
        url = "https://gcaptain.com/feed/"
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, 'xml')
        headlines = [item.title.text for item in soup.find_all('item')[:4]]
        return headlines
    except:
        return ["Unable to fetch live news. Using cached trade data."]

def get_ai_insight(headlines):
    prompt = f"Analyze these logistics headlines for 2026: {headlines}. What is the #1 risk for global trade today?"
    response = ollama.chat(model='llama3', messages=[{'role': 'user', 'content': prompt}])
    return response['message']['content']

def generate_process_map():
    # Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§ÛŒ Ø±ÙˆÛŒØ¯Ø§Ø¯ ÙØ±Ø¶ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ø¨Ø®Ø´ Process Mining
    event_data = {
        'case:concept:name': ['S1', 'S1', 'S1', 'S2', 'S2', 'S3', 'S3', 'S3', 'S3'],
        'concept:name': ['Order Picked', 'Customs Clearance', 'Delivered', 
                         'Order Picked', 'Delivered',
                         'Order Picked', 'Customs Clearance', 'Warehouse Hold', 'Delivered'],
        'time:timestamp': pd.to_datetime(['2026-01-20 08:00', '2026-01-21 10:00', '2026-01-23 15:00',
                                         '2026-01-20 09:00', '2026-01-22 14:00',
                                         '2026-01-20 08:30', '2026-01-21 11:00', '2026-01-22 09:00', '2026-01-25 10:00'])
    }
    df_event = pd.DataFrame(event_data)
    dfg, start_act, end_act = pm4py.discover_directly_follows_graph(df_event)
    pm4py.save_vis_dfg(dfg, start_act, end_act, "process_map.png")
    return "process_map.png"

# --- Û³. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ (ML) ---
# Ø­ØªÙ…Ø§Ù‹ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ø§ÛŒÙ† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ù‡Ø³ØªÙ†Ø¯
try:
    model = joblib.load('logistic_delay_model.pkl')
    model_columns = joblib.load('model_columns.pkl')
except:
    st.error("Model files not found! Please check .pkl files.")

# --- Û´. Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ (UI) ---
st.title("ğŸŒ Logistics Intelligence Hub: AI & Process Analytics 2026")
st.markdown("---")

# Ø§ÛŒØ¬Ø§Ø¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
col_left, col_right = st.columns([1, 1.5], gap="large")

# Ø³ØªÙˆÙ† Ø³Ù…Øª Ú†Ù¾: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¹Ø¯Ø¯ÛŒ (ML)
with col_left:
    st.header("ğŸ“Š Predictive ML Analysis")
    st.write("Calculate delay probability based on historical data.")
    
    ship_type = st.selectbox("Payment Type", ["DEBIT", "TRANSFER", "CASH"])
    region = st.selectbox("Region", ["Western Europe", "Central America", "Southeast Asia"])
    
    if st.button("Calculate ML Risk", use_container_width=True):
        # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ú©Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ Ù…Ø¯Ù„ Ø±Ø§ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯ØŒ ÙØ¹Ù„Ø§Ù‹ Ù†Ù…Ø§ÛŒØ´ Ø¹Ø¯Ø¯ ØªØ³ØªÛŒ:
        st.metric("Delay Probability", "72%")
        st.warning("High risk detected based on historical bottlenecks.")

# Ø³ØªÙˆÙ† Ø³Ù…Øª Ø±Ø§Ø³Øª: Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø²Ù†Ø¯Ù‡ Ùˆ ØªØ­Ù„ÛŒÙ„ ÙØ±Ø¢ÛŒÙ†Ø¯
with col_right:
    # Ø¨Ø®Ø´ Ø§ÙˆÙ„: ØªØ­Ù„ÛŒÙ„ Ù„Ø§Ù…Ø§ Û³
    st.header("ğŸ§  Generative AI Agent (Llama 3)")
    if st.button("Fetch & Analyze Live Global Risks"):
        with st.spinner("Mining 2026 Trade Data..."):
            news = fetch_live_news()
            insight = get_ai_insight(news)
            
            st.subheader("ğŸ“° Latest Global Headlines")
            for h in news:
                st.write(f"â€¢ {h}")
            
            st.info(f"**AI Strategic Report:** {insight}")

    st.markdown("---")
    
    # Ø¨Ø®Ø´ Ø¯ÙˆÙ…: ÙØ±Ø¢ÛŒÙ†Ø¯Ú©Ø§ÙˆÛŒ (Process Mining) - Ù…ÙˆØ±Ø¯ Ø¹Ù„Ø§Ù‚Ù‡ Ù¾Ø±ÙˆÙØ³ÙˆØ±
    st.header("ğŸ“‰ Business Process Analytics")
    if st.button("Analyze Process Flow & Bottlenecks"):
        with st.spinner("Generating Process Map..."):
            img_path = generate_process_map()
            st.image(img_path, caption="Directly Follows Graph (DFG) - Logistics Bottlenecks")
            st.success("Analysis: 'Warehouse Hold' identified as the primary process delay factor.")

# Ù¾Ø§ÙˆØ±Ù‚ÛŒ
st.markdown("---")
st.caption("Developed by Ilya Jafari | Research Framework: AI in Management & Process Analytics")